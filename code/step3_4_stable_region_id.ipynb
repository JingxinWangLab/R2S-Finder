{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3-4: identify the stable regions, stem-loops, bulges and internal loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas == 0.24.2\n",
      "numpy == 1.16.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import shutil\n",
    "print(\"pandas == {}\".format(pd.__version__))\n",
    "print(\"numpy == {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to recognize stem-loops, bulges and internal loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to identify any m x n bulges/internal loops\n",
    "def find_m_n_bulge(df, start_line, end_line, m, n):\n",
    "    # Adjust start_line and end_line from 1-based line numbers to 0-based indices\n",
    "    start_index = start_line - 1\n",
    "    end_index = end_line - 1\n",
    "\n",
    "    # Ensure the range is within the DataFrame index bounds\n",
    "    if start_index < df.index.min() or end_index > df.index.max():\n",
    "        raise ValueError(\"Start or end line number is out of bounds\")\n",
    "\n",
    "    # Slice the DataFrame to only consider the given line number range\n",
    "    sub_df = df.iloc[start_index:end_index + 1, :]\n",
    "    #print(sub_df)\n",
    "    # List to store the sequences of four consecutive decreasing numbers\n",
    "    sequence_num = []\n",
    "\n",
    "    # Iterate through the sliced DataFrame to find sequences\n",
    "    for i in range(len(sub_df) - (3 + m)):  # adjust range to account for m\n",
    "        # Additional condition check for zeros if m >= 1\n",
    "        if m >= 1 and not all(sub_df.iloc[i + 2:i + 2 + m][\"pair_num\"] == 0):\n",
    "            continue  # Skip this iteration if the condition is not met\n",
    "        if sub_df.iloc[i][\"pair_num\"] < sub_df.iloc[i][\"nuc_num\"]:\n",
    "            continue  # This line will avoid counting the same bulge twice!!\n",
    "        # Check the sequence of four numbers considering m and n\n",
    "        if (sub_df.iloc[i][\"pair_num\"] == sub_df.iloc[i + 1][\"pair_num\"] + 1 and\n",
    "            sub_df.iloc[i + 1][\"pair_num\"] == sub_df.iloc[i + 2 + m][\"pair_num\"] + 1 + n and\n",
    "            sub_df.iloc[i + 2 + m][\"pair_num\"] == sub_df.iloc[i + 3 + m][\"pair_num\"] + 1 and \n",
    "            sub_df.iloc[i][\"nuc_num\"] + m + 3 != sub_df.iloc[i][\"pair_num\"]):\n",
    "            # Generate number ranges\n",
    "            number_range1 = list(range(sub_df.iloc[i][\"nuc_num\"] + 2, sub_df.iloc[i][\"nuc_num\"] + m + 2))\n",
    "            number_range2 = list(range(sub_df.iloc[i][\"pair_num\"] - 2, sub_df.iloc[i][\"pair_num\"] - 2 - n, -1))\n",
    "            bulge_id = \"B{}\".format(sub_df.iloc[i][\"nuc_num\"] + 2)\n",
    "            for number in number_range1:\n",
    "                sequence_num.append((number, df.at[number-1, 'nuc_id'], bulge_id))\n",
    "            for number in number_range2:\n",
    "                sequence_num.append((number, df.at[number-1, 'nuc_id'], bulge_id))\n",
    "    return sequence_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to list all bulges/internal loops in certain range\n",
    "def bulge_search(df, start_line, end_line):\n",
    "    bulge_results = []\n",
    "    for m in range(11):\n",
    "        for n in range(11):\n",
    "            if m==0 and n==0:\n",
    "                continue\n",
    "            sequence_num = find_m_n_bulge(df, start_line, end_line, m, n)\n",
    "            if sequence_num:\n",
    "                bulge_results.append({\"m\": m, \"n\": n, \"sequence_num\": sequence_num})\n",
    "    return bulge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to identify any stem-loop with m-nucleotide loop\n",
    "def find_m_loop(df, start_line, end_line, m=4):\n",
    "    # Adjust start_line and end_line from 1-based line numbers to 0-based indices\n",
    "    start_index = start_line - 1\n",
    "    end_index = end_line - 1\n",
    "\n",
    "    # Ensure the range is within the DataFrame index bounds\n",
    "    if start_index < df.index.min() or end_index > df.index.max():\n",
    "        raise ValueError(\"Start or end line number is out of bounds\")\n",
    "\n",
    "    # Slice the DataFrame to only consider the given line number range\n",
    "    sub_df = df.iloc[start_index:end_index + 1, :]\n",
    "    #print(sub_df)\n",
    "    # List to store the sequences of four consecutive decreasing numbers\n",
    "    sequence_num = []\n",
    "\n",
    "    # Iterate through the sliced DataFrame to find sequences\n",
    "    for i in range(len(sub_df) - (3 + m)):  # adjust range to account for m\n",
    "        # Additional condition check for zeros\n",
    "        if not all(sub_df.iloc[i + 2:i + 2 + m][\"pair_num\"] == 0):\n",
    "            continue  # Skip this iteration if the condition is not met\n",
    "        # Check the loop\n",
    "        if (sub_df.iloc[i][\"pair_num\"] == sub_df.iloc[i + 1][\"pair_num\"] + 1 and\n",
    "            sub_df.iloc[i][\"pair_num\"] == sub_df.iloc[i][\"nuc_num\"] + 3 + m and\n",
    "            sub_df.iloc[i + 1][\"pair_num\"] == sub_df.iloc[i + 1][\"nuc_num\"] + 1 + m):\n",
    "            # Generate number ranges\n",
    "            number_range1 = list(range(sub_df.iloc[i][\"nuc_num\"] + 2, sub_df.iloc[i][\"nuc_num\"] + m + 2))\n",
    "            # Append results to sequence_num\n",
    "            loop_id = \"L{}\".format(sub_df.iloc[i][\"nuc_num\"] + 2)            \n",
    "            for number in number_range1:\n",
    "                sequence_num.append((number, df.at[number-1, 'nuc_id'], loop_id))\n",
    "    sequence_num = list(set(sequence_num))\n",
    "    return sequence_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to list all stem-loops in certain range\n",
    "def loop_search(df, start_line, end_line):\n",
    "    loop_results = []\n",
    "    for m in range(1, 11):\n",
    "        sequence_num = find_m_loop(df, start_line, end_line, m)\n",
    "        if sequence_num:\n",
    "            loop_results.append({\"m\": m, \"sequence_num\": sequence_num})\n",
    "    return loop_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to search for stable regions that contain at least one stem-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_stable_region(transcript_name, threshold_dp=1, threshold_c1=0, threshold_c2=0.301): \n",
    "    pattern = './data/results_{0}.map_*/merged_{0}.map_*.dp'.format(transcript_name)\n",
    "    file_list = glob.glob(pattern)\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError(\"No files found for pattern {0}\".format(pattern))\n",
    "    elif len(file_list) > 1:\n",
    "        raise Exception(\"Multiple files found for pattern {0}, please specify further.\".format(pattern))\n",
    "    file_path = file_list[0]\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=1)\n",
    "    # Filter data based on the '-log10(Probability)' threshold\n",
    "    df_rel = df[df['-log10(Probability)'] < threshold_dp]\n",
    "    df_rel = df_rel.reset_index(drop=True)\n",
    "    # Initialize columns for calculations\n",
    "    df_rel['c1'] = 0 # c1 defines if a single structure/conformer is strictly predicted\n",
    "    df_rel['c2'] = df_rel['-log10(Probability)'] # c2 defines the average probability within the range\n",
    "    df_rel['stable_closing'] = 0 # determine good closing pairs >= 3 bp\n",
    "    # Calculate 'c1' and 'c2' as specified\n",
    "    for index, row in df_rel.iterrows():\n",
    "        a = row['i']\n",
    "        b = row['j']\n",
    "        probabilities = []\n",
    "        if (\n",
    "            ((df_rel['i'] == a + 1) & (df_rel['j'] == b - 1)).any() and\n",
    "            ((df_rel['i'] == a + 2) & (df_rel['j'] == b - 2)).any() and\n",
    "            ((df_rel['i'] == a + 3) & (df_rel['j'] == b - 3)).any()\n",
    "        ):\n",
    "            df_rel.at[index, 'stable_closing'] += 1\n",
    "        for _, row2 in df_rel.iterrows():\n",
    "            a2 = row2['i']\n",
    "            b2 = row2['j']\n",
    "            if not ((a2 == a) and (b2 == b)): # targeting other rows \n",
    "                if (a <= a2 <= b) and not (a <= b2 <= b): # test if any alternative bp outside range\n",
    "                    df_rel.at[index, 'c1'] += 1\n",
    "                if ((a2 == a) and (a < b2 < b)) | ((a < a2 < b) and (b2 == b)): # test if any alternative bp within range\n",
    "                    df_rel.at[index, 'c1'] += 1\n",
    "            if a <= a2 <= b or a <= b2 <= b: # \"<=\" ensure to include the row in the first for loop\n",
    "                probabilities.append(row2['-log10(Probability)'])\n",
    "        if probabilities:\n",
    "            avg_probability = np.mean(probabilities)\n",
    "            df_rel.at[index, 'c2'] = avg_probability\n",
    "    # Calculate 'c_combine'\n",
    "    df_rel['c_combine'] = 0\n",
    "    for index, row in df_rel.iterrows():\n",
    "        a = row['i']\n",
    "        b = row['j']\n",
    "        c_count = 0\n",
    "        for index2, row2 in df_rel.iterrows():\n",
    "            if a <= row2['i'] <= b and a <= row2['j'] <= b:\n",
    "                if row2['c1'] > threshold_c1:\n",
    "                    c_count += 3\n",
    "                if row2['c2'] > threshold_c2:\n",
    "                    c_count += 1  # can tolerate low probability count twice\n",
    "        df_rel.at[index, 'c_combine'] = c_count\n",
    "    df_filt = df_rel[(df_rel['c_combine'] <= 1) & (df_rel['stable_closing'] == 1)]\n",
    "    df_filt = df_filt.reset_index(drop=True)\n",
    "    # Calculate 'max_range'; See if the range is covered by a larger region\n",
    "    df_filt['max_range'] = 0\n",
    "    for index, row in df_filt.iterrows():\n",
    "        a = row['i']\n",
    "        b = row['j']\n",
    "        max_range_count = 0\n",
    "        for index2, other_row in df_filt.iterrows():\n",
    "            if index != index2:\n",
    "                if other_row['i'] <= a and other_row['j'] >= b:\n",
    "                    max_range_count += 1\n",
    "        df_filt.at[index, 'max_range'] = max_range_count\n",
    "    df_filt2 = df_filt[df_filt['max_range'] == 0]\n",
    "    df_filt2 = df_filt2.reset_index(drop=True)\n",
    "    # Code for searching for desired bulges\n",
    "    pattern2 = './data/results_%s.map_*/merged_%s.map_*.ct' % (transcript_name, transcript_name)\n",
    "    file_list2 = glob.glob(pattern2)\n",
    "    if not file_list2:\n",
    "        raise FileNotFoundError(\"No files found for pattern: \" + pattern)\n",
    "    elif len(file_list2) > 1:\n",
    "        raise Exception(\"Multiple files found for pattern: \" + pattern + \", please specify further.\")\n",
    "    file_path2 = file_list2[0]\n",
    "    fold_df = pd.read_csv(file_path2, sep='\\\\s+', header=None, skiprows=1) ## changed \\s into \\\\s ##\n",
    "    fold_df = fold_df.drop(columns=[2, 3, 5])\n",
    "    fold_df.columns = [\"nuc_num\", \"nuc_id\", \"pair_num\"]\n",
    "    bulge_seqs = []\n",
    "    loop_seqs = []\n",
    "    df_filt2['basepair'] = 0 \n",
    "    for index, row in df_filt2.iterrows():\n",
    "        a = int(row['i'])  # Do not need to adjust for zero-based index\n",
    "        b = int(row['j'])\n",
    "        bulge_seq = bulge_search(fold_df, a, b)\n",
    "        flat_bulge_seq = [item for sublist in bulge_seq for item in sublist['sequence_num']]\n",
    "        loop_seq = loop_search(fold_df, a, b)\n",
    "        flat_loop_seq = [item for sublist in loop_seq for item in sublist['sequence_num']]\n",
    "        if len(flat_loop_seq) ==0: # At least a loop should be found in the secondary structure!\n",
    "            df_filt2.loc[index, 'basepair'] = 0 \n",
    "            continue\n",
    "        df_filt2.loc[index, 'basepair'] = b - a + 1 - len(flat_bulge_seq) - len(flat_loop_seq)\n",
    "        if df_filt2.loc[index, 'basepair'] < 8: # Need at least 4 base pairs throughout the region\n",
    "            continue\n",
    "        bulge_seqs.append(bulge_seq)\n",
    "        loop_seqs.append(loop_seq)\n",
    "    # Drop rows where 'validate' column is equal to 0\n",
    "    df_filt2 = df_filt2[df_filt2['basepair'] >= 8]\n",
    "    bulge_seqs = [element for element in bulge_seqs if element] # Drop empty elements\n",
    "    bulge_flist = [item for sublist in bulge_seqs for item in sublist] # Flatten the list to only one element\n",
    "    loop_seqs = [element for element in loop_seqs if element]\n",
    "    loop_flist = [item for sublist in loop_seqs for item in sublist]\n",
    "    # End for searching bulges\n",
    "    df_map_path = './data/{0}.map'.format(transcript_name)\n",
    "    df_map = pd.read_csv(df_map_path, sep='\\t', header=None)\n",
    "    formatted_lines = []\n",
    "    for index, row in df_filt2.iterrows():\n",
    "        a = int(row['i']) - 1\n",
    "        b = int(row['j'])\n",
    "        extracted_values = df_map.iloc[a:b, [3]].values.T  # Transpose the values\n",
    "        sequence = ''.join(map(str, extracted_values.flatten()))  # Flatten and join without tabs\n",
    "        formatted_lines.append([a + 1, b, sequence])\n",
    "    output_file_path = './data/{0}_stableseq.csv'.format(transcript_name)\n",
    "    with open(output_file_path, 'w') as f: \n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['a', 'b', 'sequence'])  # Write the header\n",
    "        writer.writerows(formatted_lines)  # Write all rows\n",
    "    \n",
    "    bulge_file_path = './data/{0}_bulge.csv'.format(transcript_name)\n",
    "    with open(bulge_file_path, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        flat_list = []\n",
    "        for item in bulge_flist:\n",
    "            m_value = item['m']\n",
    "            n_value = item.get('n', '')  # Get 'n' value if it exists, else use an empty string\n",
    "            for seq in item['sequence_num']:\n",
    "                flat_list.append((m_value, n_value, seq[0], seq[1], seq[2]))  # Append m, n, number, nuc_id\n",
    "        writer.writerow(['m', 'n', 'number', 'nuc_id', 'bulge_id'])  # Write header\n",
    "        writer.writerows(flat_list)\n",
    "    \n",
    "    loop_file_path = './data/{0}_loop.csv'.format(transcript_name)\n",
    "    with open(loop_file_path, 'w') as f:  # Use 'w' mode for binary writing in Python 2\n",
    "        writer = csv.writer(f)\n",
    "        flat_list = []\n",
    "        for item in loop_flist:\n",
    "            m_value = item['m']\n",
    "            for seq in item['sequence_num']:\n",
    "                flat_list.append((m_value, seq[0], seq[1], seq[2]))  # Append m, number, nuc_id\n",
    "        writer.writerow(['m', 'number', 'nuc_id', 'loop_id'])  # Write header\n",
    "        writer.writerows(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human invivo data\n",
    "# Define the folder path and the output CSV file name\n",
    "folder_name = \"human_invivo_map\"\n",
    "data_folder = os.path.join('./', folder_name + '_result')\n",
    "output_csv = os.path.join(data_folder, folder_name + '_list.csv')\n",
    "\n",
    "# Get the list of all *.map file names in the data folder, excluding the .map extension\n",
    "map_files = [os.path.splitext(f)[0] for f in os.listdir(data_folder) if f.endswith('.map')]\n",
    "\n",
    "# Write the list to a CSV file without a header\n",
    "with open(output_csv, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for map_file in map_files:\n",
    "        csvwriter.writerow([map_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"human_invivo_map_result\" into \"data\"\n",
    "for map_file in tqdm(map_files):\n",
    "    search_stable_region(map_file, 1, 0, 0.301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"data\"\n",
    "new_folder_name = os.path.join(base_folder, 'data_' + folder_name)\n",
    "if not os.path.exists(new_folder_name):\n",
    "    os.makedirs(new_folder_name)\n",
    "# Move all .csv and .map files into the new folder\n",
    "for filename in os.listdir(base_folder):\n",
    "    if filename.endswith('.csv') or filename.endswith('.map'):\n",
    "        source = os.path.join(base_folder, filename)\n",
    "        destination = os.path.join(new_folder_name, filename)\n",
    "        shutil.move(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human invitro data\n",
    "# Define the folder path and the output CSV file name\n",
    "folder_name = \"human_invitro_map\"\n",
    "data_folder = os.path.join('./', folder_name + '_result')\n",
    "output_csv = os.path.join(data_folder, folder_name + '_list.csv')\n",
    "\n",
    "# Get the list of all *.map file names in the data folder, excluding the .map extension\n",
    "map_files = [os.path.splitext(f)[0] for f in os.listdir(data_folder) if f.endswith('.map')]\n",
    "\n",
    "# Write the list to a CSV file without a header\n",
    "with open(output_csv, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    for map_file in map_files:\n",
    "        csvwriter.writerow([map_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename \"human_invitro_map_result\" into \"data\"\n",
    "for map_file in tqdm(map_files):\n",
    "    search_stable_region(map_file, 1, 0, 0.301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"data\"\n",
    "new_folder_name = os.path.join(base_folder, 'data_' + folder_name)\n",
    "if not os.path.exists(new_folder_name):\n",
    "    os.makedirs(new_folder_name)\n",
    "# Move all .csv and .map files into the new folder\n",
    "for filename in os.listdir(base_folder):\n",
    "    if filename.endswith('.csv') or filename.endswith('.map'):\n",
    "        source = os.path.join(base_folder, filename)\n",
    "        destination = os.path.join(new_folder_name, filename)\n",
    "        shutil.move(source, destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
